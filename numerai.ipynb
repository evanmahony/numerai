{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"numerai_training_data.csv\")\n",
    "og_df = df\n",
    "\n",
    "if df.data_type.unique() == \"train\":\n",
    "    df.drop(\"data_type\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"id\", inplace=True)\n",
    "#df[\"erano\"] = df.era.str.slice(3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"extreme_target\"] = df[\"target\"].apply(lambda x: 1 if x == 1 or x==0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     era  feature_intelligence1  feature_intelligence2  \\\n",
       "id                                                                       \n",
       "n000315175b67977    era1                   0.00                   0.50   \n",
       "n0014af834a96cdd    era1                   0.00                   0.00   \n",
       "n001c93979ac41d4    era1                   0.25                   0.50   \n",
       "n0034e4143f22a13    era1                   1.00                   0.00   \n",
       "n00679d1a636062f    era1                   0.25                   0.25   \n",
       "...                  ...                    ...                    ...   \n",
       "nff6a8a8feaeeb52  era120                   0.50                   0.50   \n",
       "nff6af62a0996372  era120                   1.00                   0.00   \n",
       "nff9288983b8c040  era120                   0.75                   0.50   \n",
       "nffaab4e1cacc4b1  era120                   0.25                   0.25   \n",
       "nffba5460b572cfa  era120                   0.75                   0.50   \n",
       "\n",
       "                  feature_intelligence3  feature_intelligence4  \\\n",
       "id                                                               \n",
       "n000315175b67977                   0.25                   0.00   \n",
       "n0014af834a96cdd                   0.00                   0.25   \n",
       "n001c93979ac41d4                   0.25                   0.25   \n",
       "n0034e4143f22a13                   0.00                   0.50   \n",
       "n00679d1a636062f                   0.25                   0.25   \n",
       "...                                 ...                    ...   \n",
       "nff6a8a8feaeeb52                   0.25                   0.00   \n",
       "nff6af62a0996372                   0.00                   1.00   \n",
       "nff9288983b8c040                   0.50                   0.50   \n",
       "nffaab4e1cacc4b1                   0.25                   0.50   \n",
       "nffba5460b572cfa                   0.50                   0.75   \n",
       "\n",
       "                  feature_intelligence5  feature_intelligence6  \\\n",
       "id                                                               \n",
       "n000315175b67977                   0.50                   0.25   \n",
       "n0014af834a96cdd                   0.50                   0.00   \n",
       "n001c93979ac41d4                   1.00                   0.75   \n",
       "n0034e4143f22a13                   0.50                   0.25   \n",
       "n00679d1a636062f                   0.00                   0.25   \n",
       "...                                 ...                    ...   \n",
       "nff6a8a8feaeeb52                   0.00                   0.50   \n",
       "nff6af62a0996372                   0.50                   0.75   \n",
       "nff9288983b8c040                   0.25                   0.50   \n",
       "nffaab4e1cacc4b1                   0.00                   1.00   \n",
       "nffba5460b572cfa                   0.75                   0.00   \n",
       "\n",
       "                  feature_intelligence7  feature_intelligence8  \\\n",
       "id                                                               \n",
       "n000315175b67977                   0.25                   0.25   \n",
       "n0014af834a96cdd                   0.00                   0.25   \n",
       "n001c93979ac41d4                   0.75                   0.25   \n",
       "n0034e4143f22a13                   0.25                   0.75   \n",
       "n00679d1a636062f                   0.50                   0.25   \n",
       "...                                 ...                    ...   \n",
       "nff6a8a8feaeeb52                   0.75                   0.00   \n",
       "nff6af62a0996372                   0.75                   1.00   \n",
       "nff9288983b8c040                   0.25                   0.50   \n",
       "nffaab4e1cacc4b1                   1.00                   0.50   \n",
       "nffba5460b572cfa                   0.00                   0.75   \n",
       "\n",
       "                  feature_intelligence9  ...  feature_wisdom39  \\\n",
       "id                                       ...                     \n",
       "n000315175b67977                   0.75  ...              1.00   \n",
       "n0014af834a96cdd                   0.50  ...              1.00   \n",
       "n001c93979ac41d4                   0.00  ...              0.50   \n",
       "n0034e4143f22a13                   0.25  ...              1.00   \n",
       "n00679d1a636062f                   0.25  ...              0.75   \n",
       "...                                 ...  ...               ...   \n",
       "nff6a8a8feaeeb52                   0.75  ...              0.50   \n",
       "nff6af62a0996372                   0.00  ...              1.00   \n",
       "nff9288983b8c040                   0.25  ...              0.75   \n",
       "nffaab4e1cacc4b1                   0.25  ...              0.75   \n",
       "nffba5460b572cfa                   0.00  ...              0.50   \n",
       "\n",
       "                  feature_wisdom40  feature_wisdom41  feature_wisdom42  \\\n",
       "id                                                                       \n",
       "n000315175b67977              0.75              0.50              0.75   \n",
       "n0014af834a96cdd              0.00              0.00              0.75   \n",
       "n001c93979ac41d4              0.00              0.00              0.50   \n",
       "n0034e4143f22a13              0.75              0.75              1.00   \n",
       "n00679d1a636062f              0.25              0.50              0.75   \n",
       "...                            ...               ...               ...   \n",
       "nff6a8a8feaeeb52              0.75              0.50              0.50   \n",
       "nff6af62a0996372              1.00              1.00              1.00   \n",
       "nff9288983b8c040              0.25              1.00              1.00   \n",
       "nffaab4e1cacc4b1              0.75              0.75              0.75   \n",
       "nffba5460b572cfa              0.25              0.50              0.75   \n",
       "\n",
       "                  feature_wisdom43  feature_wisdom44  feature_wisdom45  \\\n",
       "id                                                                       \n",
       "n000315175b67977              0.50              1.00              0.50   \n",
       "n0014af834a96cdd              0.25              0.00              0.25   \n",
       "n001c93979ac41d4              1.00              0.00              0.25   \n",
       "n0034e4143f22a13              1.00              0.75              1.00   \n",
       "n00679d1a636062f              0.00              0.50              0.25   \n",
       "...                            ...               ...               ...   \n",
       "nff6a8a8feaeeb52              0.75              0.25              0.25   \n",
       "nff6af62a0996372              0.00              0.75              1.00   \n",
       "nff9288983b8c040              1.00              0.25              0.00   \n",
       "nffaab4e1cacc4b1              0.50              0.50              0.25   \n",
       "nffba5460b572cfa              1.00              0.25              0.75   \n",
       "\n",
       "                  feature_wisdom46  target  extreme_target  \n",
       "id                                                          \n",
       "n000315175b67977              0.75    0.50               0  \n",
       "n0014af834a96cdd              1.00    0.25               0  \n",
       "n001c93979ac41d4              0.75    0.25               0  \n",
       "n0034e4143f22a13              1.00    0.25               0  \n",
       "n00679d1a636062f              0.75    0.75               0  \n",
       "...                            ...     ...             ...  \n",
       "nff6a8a8feaeeb52              0.25    0.50               0  \n",
       "nff6af62a0996372              1.00    0.75               0  \n",
       "nff9288983b8c040              0.00    0.25               0  \n",
       "nffaab4e1cacc4b1              0.75    0.50               0  \n",
       "nffba5460b572cfa              0.50    0.50               0  \n",
       "\n",
       "[501808 rows x 313 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>era</th>\n      <th>feature_intelligence1</th>\n      <th>feature_intelligence2</th>\n      <th>feature_intelligence3</th>\n      <th>feature_intelligence4</th>\n      <th>feature_intelligence5</th>\n      <th>feature_intelligence6</th>\n      <th>feature_intelligence7</th>\n      <th>feature_intelligence8</th>\n      <th>feature_intelligence9</th>\n      <th>...</th>\n      <th>feature_wisdom39</th>\n      <th>feature_wisdom40</th>\n      <th>feature_wisdom41</th>\n      <th>feature_wisdom42</th>\n      <th>feature_wisdom43</th>\n      <th>feature_wisdom44</th>\n      <th>feature_wisdom45</th>\n      <th>feature_wisdom46</th>\n      <th>target</th>\n      <th>extreme_target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n000315175b67977</th>\n      <td>era1</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>n0014af834a96cdd</th>\n      <td>era1</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>n001c93979ac41d4</th>\n      <td>era1</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>n0034e4143f22a13</th>\n      <td>era1</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>n00679d1a636062f</th>\n      <td>era1</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>nff6a8a8feaeeb52</th>\n      <td>era120</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>nff6af62a0996372</th>\n      <td>era120</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>nff9288983b8c040</th>\n      <td>era120</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>0.25</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>nffaab4e1cacc4b1</th>\n      <td>era120</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>nffba5460b572cfa</th>\n      <td>era120</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.50</td>\n      <td>0.25</td>\n      <td>0.50</td>\n      <td>0.75</td>\n      <td>1.00</td>\n      <td>0.25</td>\n      <td>0.75</td>\n      <td>0.50</td>\n      <td>0.50</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>501808 rows × 313 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Count of value 0 is 29180532\n",
      "Proportion of value 0 is 0.18758319593768288\n",
      "Count of value 0.25 is 29158468\n",
      "Proportion of value 0.25 is 0.1874413604277899\n",
      "Count of value 0.5 is 38889796\n",
      "Proportion of value 0.5 is 0.24999791720879236\n",
      "Count of value 0.75 is 29158468\n",
      "Proportion of value 0.75 is 0.1874413604277899\n",
      "Count of value 1 is 29173216\n",
      "Proportion of value 1 is 0.18753616599794498\n"
     ]
    }
   ],
   "source": [
    "data = df.iloc[:,1:311].values\n",
    "size = data.shape[0] * data.shape[1]\n",
    "\n",
    "for value in [0, 0.25, 0.5, 0.75, 1]:\n",
    "    count = np.array(df.iloc[:,1:311].values == value).sum()\n",
    "    print(f\"Count of value {value} is {count}\")\n",
    "    print(f\"Proportion of value {value} is {count/size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target\n0.00     25016\n0.25    100053\n0.50    251677\n0.75    100045\n1.00     25017\ndtype: int64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'target'}>]], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 394.375 263.63625\" width=\"394.375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-07-06T23:42:42.398843</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 394.375 263.63625 \nL 394.375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 52.375 239.758125 \nL 387.175 239.758125 \nL 387.175 22.318125 \nL 52.375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 67.593182 239.758125 \nL 98.029545 239.758125 \nL 98.029545 219.174376 \nL 67.593182 219.174376 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 98.029545 239.758125 \nL 128.465909 239.758125 \nL 128.465909 239.758125 \nL 98.029545 239.758125 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 128.465909 239.758125 \nL 158.902273 239.758125 \nL 158.902273 157.43218 \nL 128.465909 157.43218 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 158.902273 239.758125 \nL 189.338636 239.758125 \nL 189.338636 239.758125 \nL 158.902273 239.758125 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 189.338636 239.758125 \nL 219.775 239.758125 \nL 219.775 239.758125 \nL 189.338636 239.758125 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 219.775 239.758125 \nL 250.211364 239.758125 \nL 250.211364 32.672411 \nL 219.775 32.672411 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 250.211364 239.758125 \nL 280.647727 239.758125 \nL 280.647727 239.758125 \nL 250.211364 239.758125 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 280.647727 239.758125 \nL 311.084091 239.758125 \nL 311.084091 157.438762 \nL 280.647727 157.438762 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 311.084091 239.758125 \nL 341.520455 239.758125 \nL 341.520455 239.758125 \nL 311.084091 239.758125 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p640d77bf1e)\" d=\"M 341.520455 239.758125 \nL 371.956818 239.758125 \nL 371.956818 219.173553 \nL 341.520455 219.173553 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 67.593182 239.758125 \nL 67.593182 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m99252975ec\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"67.593182\" xlink:href=\"#m99252975ec\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(59.641619 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 128.465909 239.758125 \nL 128.465909 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.465909\" xlink:href=\"#m99252975ec\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <g transform=\"translate(120.514347 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 189.338636 239.758125 \nL 189.338636 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"189.338636\" xlink:href=\"#m99252975ec\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <g transform=\"translate(181.387074 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 250.211364 239.758125 \nL 250.211364 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.211364\" xlink:href=\"#m99252975ec\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <g transform=\"translate(242.259801 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 311.084091 239.758125 \nL 311.084091 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.084091\" xlink:href=\"#m99252975ec\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <g transform=\"translate(303.132528 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 371.956818 239.758125 \nL 371.956818 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"371.956818\" xlink:href=\"#m99252975ec\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <g transform=\"translate(364.005256 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 52.375 239.758125 \nL 387.175 239.758125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m518bebe6d0\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#m518bebe6d0\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(39.0125 243.557344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 52.375 198.616957 \nL 387.175 198.616957 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#m518bebe6d0\" y=\"198.616957\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50000 -->\n      <g transform=\"translate(13.5625 202.416176)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 52.375 157.475789 \nL 387.175 157.475789 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#m518bebe6d0\" y=\"157.475789\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100000 -->\n      <g transform=\"translate(7.2 161.275008)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 52.375 116.334621 \nL 387.175 116.334621 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#m518bebe6d0\" y=\"116.334621\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 150000 -->\n      <g transform=\"translate(7.2 120.13384)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 52.375 75.193453 \nL 387.175 75.193453 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#m518bebe6d0\" y=\"75.193453\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 200000 -->\n      <g transform=\"translate(7.2 78.992672)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p640d77bf1e)\" d=\"M 52.375 34.052285 \nL 387.175 34.052285 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.375\" xlink:href=\"#m518bebe6d0\" y=\"34.052285\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 250000 -->\n      <g transform=\"translate(7.2 37.851504)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 52.375 239.758125 \nL 52.375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 387.175 239.758125 \nL 387.175 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 52.375 239.758125 \nL 387.175 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 52.375 22.318125 \nL 387.175 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- target -->\n    <g transform=\"translate(201.53125 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\n      <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-74\"/>\n     <use x=\"39.208984\" xlink:href=\"#DejaVuSans-61\"/>\n     <use x=\"100.488281\" xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"139.851562\" xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"203.328125\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"264.851562\" xlink:href=\"#DejaVuSans-74\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p640d77bf1e\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"52.375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "print(df.groupby(\"target\").size())\n",
    "df.hist(column=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\", \"constitution\"]\n",
    "\n",
    "columns = df.iloc[:,1:311].columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dfs = dict()\n",
    "\n",
    "for feature in features:\n",
    "    feature_dfs[feature] = df[columns[np.array([feature in value for value in columns]) == True]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pd.DataFrame()\n",
    "\n",
    "for feature in features:\n",
    "    transformed_df[feature] = feature_dfs[feature].mean(axis=1)\n",
    "\n",
    "transformed_df[\"target\"] = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  intelligence    wisdom  charisma  dexterity  strength  \\\n",
       "id                                                                        \n",
       "n000315175b67977      0.333333  0.668478  0.438953   0.696429  0.480263   \n",
       "n0014af834a96cdd      0.208333  0.559783  0.485465   0.267857  0.407895   \n",
       "n001c93979ac41d4      0.479167  0.635870  0.529070   0.446429  0.203947   \n",
       "n0034e4143f22a13      0.416667  0.831522  0.529070   0.232143  0.394737   \n",
       "n00679d1a636062f      0.270833  0.527174  0.421512   0.500000  0.342105   \n",
       "...                        ...       ...       ...        ...       ...   \n",
       "nff6a8a8feaeeb52      0.354167  0.353261  0.427326   0.517857  0.513158   \n",
       "nff6af62a0996372      0.541667  0.809783  0.662791   0.035714  0.756579   \n",
       "nff9288983b8c040      0.437500  0.652174  0.508721   0.428571  0.486842   \n",
       "nffaab4e1cacc4b1      0.375000  0.478261  0.534884   0.464286  0.546053   \n",
       "nffba5460b572cfa      0.437500  0.619565  0.351744   0.428571  0.473684   \n",
       "\n",
       "                  constitution  target  \n",
       "id                                      \n",
       "n000315175b67977      0.427632    0.50  \n",
       "n0014af834a96cdd      0.644737    0.25  \n",
       "n001c93979ac41d4      0.418860    0.25  \n",
       "n0034e4143f22a13      0.429825    0.25  \n",
       "n00679d1a636062f      0.508772    0.75  \n",
       "...                        ...     ...  \n",
       "nff6a8a8feaeeb52      0.480263    0.50  \n",
       "nff6af62a0996372      0.677632    0.75  \n",
       "nff9288983b8c040      0.480263    0.25  \n",
       "nffaab4e1cacc4b1      0.445175    0.50  \n",
       "nffba5460b572cfa      0.414474    0.50  \n",
       "\n",
       "[501808 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intelligence</th>\n      <th>wisdom</th>\n      <th>charisma</th>\n      <th>dexterity</th>\n      <th>strength</th>\n      <th>constitution</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n000315175b67977</th>\n      <td>0.333333</td>\n      <td>0.668478</td>\n      <td>0.438953</td>\n      <td>0.696429</td>\n      <td>0.480263</td>\n      <td>0.427632</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>n0014af834a96cdd</th>\n      <td>0.208333</td>\n      <td>0.559783</td>\n      <td>0.485465</td>\n      <td>0.267857</td>\n      <td>0.407895</td>\n      <td>0.644737</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>n001c93979ac41d4</th>\n      <td>0.479167</td>\n      <td>0.635870</td>\n      <td>0.529070</td>\n      <td>0.446429</td>\n      <td>0.203947</td>\n      <td>0.418860</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>n0034e4143f22a13</th>\n      <td>0.416667</td>\n      <td>0.831522</td>\n      <td>0.529070</td>\n      <td>0.232143</td>\n      <td>0.394737</td>\n      <td>0.429825</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>n00679d1a636062f</th>\n      <td>0.270833</td>\n      <td>0.527174</td>\n      <td>0.421512</td>\n      <td>0.500000</td>\n      <td>0.342105</td>\n      <td>0.508772</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>nff6a8a8feaeeb52</th>\n      <td>0.354167</td>\n      <td>0.353261</td>\n      <td>0.427326</td>\n      <td>0.517857</td>\n      <td>0.513158</td>\n      <td>0.480263</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>nff6af62a0996372</th>\n      <td>0.541667</td>\n      <td>0.809783</td>\n      <td>0.662791</td>\n      <td>0.035714</td>\n      <td>0.756579</td>\n      <td>0.677632</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>nff9288983b8c040</th>\n      <td>0.437500</td>\n      <td>0.652174</td>\n      <td>0.508721</td>\n      <td>0.428571</td>\n      <td>0.486842</td>\n      <td>0.480263</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>nffaab4e1cacc4b1</th>\n      <td>0.375000</td>\n      <td>0.478261</td>\n      <td>0.534884</td>\n      <td>0.464286</td>\n      <td>0.546053</td>\n      <td>0.445175</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>nffba5460b572cfa</th>\n      <td>0.437500</td>\n      <td>0.619565</td>\n      <td>0.351744</td>\n      <td>0.428571</td>\n      <td>0.473684</td>\n      <td>0.414474</td>\n      <td>0.50</td>\n    </tr>\n  </tbody>\n</table>\n<p>501808 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = transformed_df.sample(frac = 0.8)\n",
    "test_df = transformed_df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  intelligence    wisdom  charisma  dexterity  strength  \\\n",
       "id                                                                        \n",
       "n9c362e173c0eedf      0.416667  0.467391  0.418605   0.892857  0.388158   \n",
       "nec259c6f2f2e96c      0.437500  0.375000  0.476744   0.107143  0.414474   \n",
       "n6d35aec834262a0      0.500000  0.521739  0.345930   0.482143  0.493421   \n",
       "n9472c6da733f5a5      0.625000  0.380435  0.709302   0.535714  0.782895   \n",
       "n162af40c985a22e      0.437500  0.336957  0.267442   0.071429  0.171053   \n",
       "...                        ...       ...       ...        ...       ...   \n",
       "n1aaa7df60034c98      0.187500  0.652174  0.389535   0.375000  0.381579   \n",
       "n62a097ecefd0256      0.375000  0.744565  0.529070   0.446429  0.328947   \n",
       "nb87503ad1b865c7      0.541667  0.184783  0.357558   0.035714  0.381579   \n",
       "neda1696282dc0cf      0.500000  0.619565  0.305233   0.660714  0.171053   \n",
       "ne0ead558d30747c      0.854167  0.141304  0.508721   0.714286  0.184211   \n",
       "\n",
       "                  constitution  target  \n",
       "id                                      \n",
       "n9c362e173c0eedf      0.535088    0.50  \n",
       "nec259c6f2f2e96c      0.583333    0.50  \n",
       "n6d35aec834262a0      0.550439    0.50  \n",
       "n9472c6da733f5a5      0.618421    0.50  \n",
       "n162af40c985a22e      0.307018    0.75  \n",
       "...                        ...     ...  \n",
       "n1aaa7df60034c98      0.517544    0.75  \n",
       "n62a097ecefd0256      0.385965    0.50  \n",
       "nb87503ad1b865c7      0.372807    0.00  \n",
       "neda1696282dc0cf      0.388158    0.25  \n",
       "ne0ead558d30747c      0.414474    0.00  \n",
       "\n",
       "[401446 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intelligence</th>\n      <th>wisdom</th>\n      <th>charisma</th>\n      <th>dexterity</th>\n      <th>strength</th>\n      <th>constitution</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>n9c362e173c0eedf</th>\n      <td>0.416667</td>\n      <td>0.467391</td>\n      <td>0.418605</td>\n      <td>0.892857</td>\n      <td>0.388158</td>\n      <td>0.535088</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>nec259c6f2f2e96c</th>\n      <td>0.437500</td>\n      <td>0.375000</td>\n      <td>0.476744</td>\n      <td>0.107143</td>\n      <td>0.414474</td>\n      <td>0.583333</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>n6d35aec834262a0</th>\n      <td>0.500000</td>\n      <td>0.521739</td>\n      <td>0.345930</td>\n      <td>0.482143</td>\n      <td>0.493421</td>\n      <td>0.550439</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>n9472c6da733f5a5</th>\n      <td>0.625000</td>\n      <td>0.380435</td>\n      <td>0.709302</td>\n      <td>0.535714</td>\n      <td>0.782895</td>\n      <td>0.618421</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>n162af40c985a22e</th>\n      <td>0.437500</td>\n      <td>0.336957</td>\n      <td>0.267442</td>\n      <td>0.071429</td>\n      <td>0.171053</td>\n      <td>0.307018</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>n1aaa7df60034c98</th>\n      <td>0.187500</td>\n      <td>0.652174</td>\n      <td>0.389535</td>\n      <td>0.375000</td>\n      <td>0.381579</td>\n      <td>0.517544</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>n62a097ecefd0256</th>\n      <td>0.375000</td>\n      <td>0.744565</td>\n      <td>0.529070</td>\n      <td>0.446429</td>\n      <td>0.328947</td>\n      <td>0.385965</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>nb87503ad1b865c7</th>\n      <td>0.541667</td>\n      <td>0.184783</td>\n      <td>0.357558</td>\n      <td>0.035714</td>\n      <td>0.381579</td>\n      <td>0.372807</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>neda1696282dc0cf</th>\n      <td>0.500000</td>\n      <td>0.619565</td>\n      <td>0.305233</td>\n      <td>0.660714</td>\n      <td>0.171053</td>\n      <td>0.388158</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>ne0ead558d30747c</th>\n      <td>0.854167</td>\n      <td>0.141304</td>\n      <td>0.508721</td>\n      <td>0.714286</td>\n      <td>0.184211</td>\n      <td>0.414474</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>401446 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f09d3976cd0>\n"
     ]
    }
   ],
   "source": [
    "train = torch.utils.data.TensorDataset(torch.from_numpy(train_df.iloc[:, :6].values), torch.from_numpy(train_df[\"target\"].values))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "401446\n(401446, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader.dataset))\n",
    "print(train_df.iloc[:, :6].values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f09d3976cd0>\n"
     ]
    }
   ],
   "source": [
    "test = torch.utils.data.TensorDataset(torch.from_numpy(test_df.iloc[:, :6].values), torch.from_numpy(test_df[\"target\"].values))\n",
    "test_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([32, 6])\nShape of y:  torch.Size([32]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\nNeuralNetwork(\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=6, out_features=36, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=36, out_features=648, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=648, out_features=36, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=36, out_features=6, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=6, out_features=1, bias=True)\n    (9): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(6, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36, 648),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(648, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(36, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x.float())\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y.type(torch.float).unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.type(torch.float).unsqueeze(1)).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "loss: 51.562500  [105600/401446]\n",
      "loss: 50.000000  [108800/401446]\n",
      "loss: 51.562500  [112000/401446]\n",
      "loss: 50.000000  [115200/401446]\n",
      "loss: 46.093750  [118400/401446]\n",
      "loss: 47.656250  [121600/401446]\n",
      "loss: 52.343750  [124800/401446]\n",
      "loss: 46.875000  [128000/401446]\n",
      "loss: 52.343750  [131200/401446]\n",
      "loss: 48.437500  [134400/401446]\n",
      "loss: 50.000000  [137600/401446]\n",
      "loss: 50.781250  [140800/401446]\n",
      "loss: 52.343750  [144000/401446]\n",
      "loss: 52.343750  [147200/401446]\n",
      "loss: 50.781250  [150400/401446]\n",
      "loss: 50.000000  [153600/401446]\n",
      "loss: 48.437500  [156800/401446]\n",
      "loss: 49.218750  [160000/401446]\n",
      "loss: 50.000000  [163200/401446]\n",
      "loss: 49.218750  [166400/401446]\n",
      "loss: 46.093750  [169600/401446]\n",
      "loss: 40.625000  [172800/401446]\n",
      "loss: 52.343750  [176000/401446]\n",
      "loss: 44.531250  [179200/401446]\n",
      "loss: 53.125000  [182400/401446]\n",
      "loss: 52.343750  [185600/401446]\n",
      "loss: 51.562500  [188800/401446]\n",
      "loss: 53.906250  [192000/401446]\n",
      "loss: 50.000000  [195200/401446]\n",
      "loss: 48.437500  [198400/401446]\n",
      "loss: 50.000000  [201600/401446]\n",
      "loss: 53.125000  [204800/401446]\n",
      "loss: 47.656250  [208000/401446]\n",
      "loss: 50.000000  [211200/401446]\n",
      "loss: 47.656250  [214400/401446]\n",
      "loss: 57.031250  [217600/401446]\n",
      "loss: 46.875000  [220800/401446]\n",
      "loss: 50.781250  [224000/401446]\n",
      "loss: 49.218750  [227200/401446]\n",
      "loss: 47.656250  [230400/401446]\n",
      "loss: 47.656250  [233600/401446]\n",
      "loss: 44.531250  [236800/401446]\n",
      "loss: 43.750000  [240000/401446]\n",
      "loss: 46.875000  [243200/401446]\n",
      "loss: 42.187500  [246400/401446]\n",
      "loss: 53.125000  [249600/401446]\n",
      "loss: 49.218750  [252800/401446]\n",
      "loss: 49.218750  [256000/401446]\n",
      "loss: 53.906250  [259200/401446]\n",
      "loss: 42.968750  [262400/401446]\n",
      "loss: 47.656250  [265600/401446]\n",
      "loss: 52.343750  [268800/401446]\n",
      "loss: 46.875000  [272000/401446]\n",
      "loss: 47.656250  [275200/401446]\n",
      "loss: 50.000000  [278400/401446]\n",
      "loss: 53.906250  [281600/401446]\n",
      "loss: 43.750000  [284800/401446]\n",
      "loss: 45.312500  [288000/401446]\n",
      "loss: 57.031250  [291200/401446]\n",
      "loss: 53.125000  [294400/401446]\n",
      "loss: 47.656250  [297600/401446]\n",
      "loss: 53.125000  [300800/401446]\n",
      "loss: 46.875000  [304000/401446]\n",
      "loss: 57.812500  [307200/401446]\n",
      "loss: 50.781250  [310400/401446]\n",
      "loss: 52.343750  [313600/401446]\n",
      "loss: 50.781250  [316800/401446]\n",
      "loss: 47.656250  [320000/401446]\n",
      "loss: 51.562500  [323200/401446]\n",
      "loss: 54.687500  [326400/401446]\n",
      "loss: 50.000000  [329600/401446]\n",
      "loss: 51.562500  [332800/401446]\n",
      "loss: 51.562500  [336000/401446]\n",
      "loss: 53.125000  [339200/401446]\n",
      "loss: 53.906250  [342400/401446]\n",
      "loss: 57.031250  [345600/401446]\n",
      "loss: 48.437500  [348800/401446]\n",
      "loss: 51.562500  [352000/401446]\n",
      "loss: 49.218750  [355200/401446]\n",
      "loss: 52.343750  [358400/401446]\n",
      "loss: 52.343750  [361600/401446]\n",
      "loss: 45.312500  [364800/401446]\n",
      "loss: 54.687500  [368000/401446]\n",
      "loss: 51.562500  [371200/401446]\n",
      "loss: 45.312500  [374400/401446]\n",
      "loss: 50.781250  [377600/401446]\n",
      "loss: 55.468750  [380800/401446]\n",
      "loss: 55.468750  [384000/401446]\n",
      "loss: 50.781250  [387200/401446]\n",
      "loss: 51.562500  [390400/401446]\n",
      "loss: 52.343750  [393600/401446]\n",
      "loss: 46.093750  [396800/401446]\n",
      "loss: 41.406250  [400000/401446]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 50.008407 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 47.656250  [    0/401446]\n",
      "loss: 50.000000  [ 3200/401446]\n",
      "loss: 44.531250  [ 6400/401446]\n",
      "loss: 50.000000  [ 9600/401446]\n",
      "loss: 50.781250  [12800/401446]\n",
      "loss: 49.218750  [16000/401446]\n",
      "loss: 47.656250  [19200/401446]\n",
      "loss: 51.562500  [22400/401446]\n",
      "loss: 47.656250  [25600/401446]\n",
      "loss: 53.125000  [28800/401446]\n",
      "loss: 56.250000  [32000/401446]\n",
      "loss: 54.687500  [35200/401446]\n",
      "loss: 50.781250  [38400/401446]\n",
      "loss: 46.093750  [41600/401446]\n",
      "loss: 44.531250  [44800/401446]\n",
      "loss: 45.312500  [48000/401446]\n",
      "loss: 46.093750  [51200/401446]\n",
      "loss: 56.250000  [54400/401446]\n",
      "loss: 49.218750  [57600/401446]\n",
      "loss: 50.781250  [60800/401446]\n",
      "loss: 50.000000  [64000/401446]\n",
      "loss: 55.468750  [67200/401446]\n",
      "loss: 48.437500  [70400/401446]\n",
      "loss: 48.437500  [73600/401446]\n",
      "loss: 46.093750  [76800/401446]\n",
      "loss: 49.218750  [80000/401446]\n",
      "loss: 40.625000  [83200/401446]\n",
      "loss: 56.250000  [86400/401446]\n",
      "loss: 50.781250  [89600/401446]\n",
      "loss: 41.406250  [92800/401446]\n",
      "loss: 47.656250  [96000/401446]\n",
      "loss: 51.562500  [99200/401446]\n",
      "loss: 57.812500  [102400/401446]\n",
      "loss: 48.437500  [105600/401446]\n",
      "loss: 60.937500  [108800/401446]\n",
      "loss: 51.562500  [112000/401446]\n",
      "loss: 50.781250  [115200/401446]\n",
      "loss: 50.781250  [118400/401446]\n",
      "loss: 52.343750  [121600/401446]\n",
      "loss: 48.437500  [124800/401446]\n",
      "loss: 56.250000  [128000/401446]\n",
      "loss: 50.000000  [131200/401446]\n",
      "loss: 52.343750  [134400/401446]\n",
      "loss: 50.000000  [137600/401446]\n",
      "loss: 53.125000  [140800/401446]\n",
      "loss: 44.531250  [144000/401446]\n",
      "loss: 56.250000  [147200/401446]\n",
      "loss: 57.031250  [150400/401446]\n",
      "loss: 57.812500  [153600/401446]\n",
      "loss: 44.531250  [156800/401446]\n",
      "loss: 45.312500  [160000/401446]\n",
      "loss: 49.218750  [163200/401446]\n",
      "loss: 47.656250  [166400/401446]\n",
      "loss: 51.562500  [169600/401446]\n",
      "loss: 43.750000  [172800/401446]\n",
      "loss: 56.250000  [176000/401446]\n",
      "loss: 45.312500  [179200/401446]\n",
      "loss: 55.468750  [182400/401446]\n",
      "loss: 50.781250  [185600/401446]\n",
      "loss: 47.656250  [188800/401446]\n",
      "loss: 46.875000  [192000/401446]\n",
      "loss: 52.343750  [195200/401446]\n",
      "loss: 47.656250  [198400/401446]\n",
      "loss: 48.437500  [201600/401446]\n",
      "loss: 51.562500  [204800/401446]\n",
      "loss: 50.781250  [208000/401446]\n",
      "loss: 53.125000  [211200/401446]\n",
      "loss: 51.562500  [214400/401446]\n",
      "loss: 46.875000  [217600/401446]\n",
      "loss: 46.875000  [220800/401446]\n",
      "loss: 48.437500  [224000/401446]\n",
      "loss: 45.312500  [227200/401446]\n",
      "loss: 44.531250  [230400/401446]\n",
      "loss: 53.125000  [233600/401446]\n",
      "loss: 53.125000  [236800/401446]\n",
      "loss: 44.531250  [240000/401446]\n",
      "loss: 39.843750  [243200/401446]\n",
      "loss: 54.687500  [246400/401446]\n",
      "loss: 45.312500  [249600/401446]\n",
      "loss: 54.687500  [252800/401446]\n",
      "loss: 52.343750  [256000/401446]\n",
      "loss: 56.250000  [259200/401446]\n",
      "loss: 46.875000  [262400/401446]\n",
      "loss: 50.000000  [265600/401446]\n",
      "loss: 50.000000  [268800/401446]\n",
      "loss: 47.656250  [272000/401446]\n",
      "loss: 49.218750  [275200/401446]\n",
      "loss: 50.000000  [278400/401446]\n",
      "loss: 57.812500  [281600/401446]\n",
      "loss: 58.593750  [284800/401446]\n",
      "loss: 48.437500  [288000/401446]\n",
      "loss: 48.437500  [291200/401446]\n",
      "loss: 50.000000  [294400/401446]\n",
      "loss: 50.000000  [297600/401446]\n",
      "loss: 51.562500  [300800/401446]\n",
      "loss: 53.125000  [304000/401446]\n",
      "loss: 56.250000  [307200/401446]\n",
      "loss: 53.125000  [310400/401446]\n",
      "loss: 51.562500  [313600/401446]\n",
      "loss: 53.906250  [316800/401446]\n",
      "loss: 48.437500  [320000/401446]\n",
      "loss: 51.562500  [323200/401446]\n",
      "loss: 50.781250  [326400/401446]\n",
      "loss: 53.906250  [329600/401446]\n",
      "loss: 46.875000  [332800/401446]\n",
      "loss: 45.312500  [336000/401446]\n",
      "loss: 34.375000  [339200/401446]\n",
      "loss: 45.312500  [342400/401446]\n",
      "loss: 46.093750  [345600/401446]\n",
      "loss: 45.312500  [348800/401446]\n",
      "loss: 52.343750  [352000/401446]\n",
      "loss: 46.875000  [355200/401446]\n",
      "loss: 56.250000  [358400/401446]\n",
      "loss: 44.531250  [361600/401446]\n",
      "loss: 51.562500  [364800/401446]\n",
      "loss: 53.906250  [368000/401446]\n",
      "loss: 53.906250  [371200/401446]\n",
      "loss: 53.125000  [374400/401446]\n",
      "loss: 52.343750  [377600/401446]\n",
      "loss: 53.906250  [380800/401446]\n",
      "loss: 44.531250  [384000/401446]\n",
      "loss: 51.562500  [387200/401446]\n",
      "loss: 50.000000  [390400/401446]\n",
      "loss: 49.218750  [393600/401446]\n",
      "loss: 46.875000  [396800/401446]\n",
      "loss: 50.000000  [400000/401446]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 50.008137 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 47.656250  [    0/401446]\n",
      "loss: 50.000000  [ 3200/401446]\n",
      "loss: 45.312500  [ 6400/401446]\n",
      "loss: 52.343750  [ 9600/401446]\n",
      "loss: 52.343750  [12800/401446]\n",
      "loss: 50.000000  [16000/401446]\n",
      "loss: 51.562500  [19200/401446]\n",
      "loss: 46.093750  [22400/401446]\n",
      "loss: 51.562500  [25600/401446]\n",
      "loss: 48.437500  [28800/401446]\n",
      "loss: 46.875000  [32000/401446]\n",
      "loss: 53.125000  [35200/401446]\n",
      "loss: 53.125000  [38400/401446]\n",
      "loss: 45.312500  [41600/401446]\n",
      "loss: 54.687500  [44800/401446]\n",
      "loss: 50.000000  [48000/401446]\n",
      "loss: 49.218750  [51200/401446]\n",
      "loss: 53.125000  [54400/401446]\n",
      "loss: 51.562500  [57600/401446]\n",
      "loss: 50.781250  [60800/401446]\n",
      "loss: 46.875000  [64000/401446]\n",
      "loss: 50.781250  [67200/401446]\n",
      "loss: 57.031250  [70400/401446]\n",
      "loss: 47.656250  [73600/401446]\n",
      "loss: 53.125000  [76800/401446]\n",
      "loss: 48.437500  [80000/401446]\n",
      "loss: 46.093750  [83200/401446]\n",
      "loss: 53.906250  [86400/401446]\n",
      "loss: 49.218750  [89600/401446]\n",
      "loss: 51.562500  [92800/401446]\n",
      "loss: 49.218750  [96000/401446]\n",
      "loss: 47.656250  [99200/401446]\n",
      "loss: 48.437500  [102400/401446]\n",
      "loss: 48.437500  [105600/401446]\n",
      "loss: 53.906250  [108800/401446]\n",
      "loss: 54.687500  [112000/401446]\n",
      "loss: 54.687500  [115200/401446]\n",
      "loss: 52.343750  [118400/401446]\n",
      "loss: 54.687500  [121600/401446]\n",
      "loss: 46.875000  [124800/401446]\n",
      "loss: 53.906250  [128000/401446]\n",
      "loss: 53.906250  [131200/401446]\n",
      "loss: 50.781250  [134400/401446]\n",
      "loss: 52.343750  [137600/401446]\n",
      "loss: 50.000000  [140800/401446]\n",
      "loss: 46.093750  [144000/401446]\n",
      "loss: 40.625000  [147200/401446]\n",
      "loss: 48.437500  [150400/401446]\n",
      "loss: 50.000000  [153600/401446]\n",
      "loss: 52.343750  [156800/401446]\n",
      "loss: 53.906250  [160000/401446]\n",
      "loss: 46.875000  [163200/401446]\n",
      "loss: 49.218750  [166400/401446]\n",
      "loss: 58.593750  [169600/401446]\n",
      "loss: 44.531250  [172800/401446]\n",
      "loss: 48.437500  [176000/401446]\n",
      "loss: 46.093750  [179200/401446]\n",
      "loss: 50.000000  [182400/401446]\n",
      "loss: 50.781250  [185600/401446]\n",
      "loss: 46.875000  [188800/401446]\n",
      "loss: 48.437500  [192000/401446]\n",
      "loss: 49.218750  [195200/401446]\n",
      "loss: 59.375000  [198400/401446]\n",
      "loss: 53.125000  [201600/401446]\n",
      "loss: 51.562500  [204800/401446]\n",
      "loss: 46.875000  [208000/401446]\n",
      "loss: 57.031250  [211200/401446]\n",
      "loss: 42.968750  [214400/401446]\n",
      "loss: 50.781250  [217600/401446]\n",
      "loss: 50.000000  [220800/401446]\n",
      "loss: 45.312500  [224000/401446]\n",
      "loss: 40.625000  [227200/401446]\n",
      "loss: 46.875000  [230400/401446]\n",
      "loss: 51.562500  [233600/401446]\n",
      "loss: 49.218750  [236800/401446]\n",
      "loss: 42.187500  [240000/401446]\n",
      "loss: 42.968750  [243200/401446]\n",
      "loss: 57.812500  [246400/401446]\n",
      "loss: 47.656250  [249600/401446]\n",
      "loss: 47.656250  [252800/401446]\n",
      "loss: 49.218750  [256000/401446]\n",
      "loss: 43.750000  [259200/401446]\n",
      "loss: 53.906250  [262400/401446]\n",
      "loss: 53.125000  [265600/401446]\n",
      "loss: 49.218750  [268800/401446]\n",
      "loss: 53.125000  [272000/401446]\n",
      "loss: 50.000000  [275200/401446]\n",
      "loss: 53.125000  [278400/401446]\n",
      "loss: 46.093750  [281600/401446]\n",
      "loss: 41.406250  [284800/401446]\n",
      "loss: 50.000000  [288000/401446]\n",
      "loss: 52.343750  [291200/401446]\n",
      "loss: 54.687500  [294400/401446]\n",
      "loss: 46.093750  [297600/401446]\n",
      "loss: 46.875000  [300800/401446]\n",
      "loss: 46.875000  [304000/401446]\n",
      "loss: 53.125000  [307200/401446]\n",
      "loss: 48.437500  [310400/401446]\n",
      "loss: 43.750000  [313600/401446]\n",
      "loss: 42.187500  [316800/401446]\n",
      "loss: 50.781250  [320000/401446]\n",
      "loss: 44.531250  [323200/401446]\n",
      "loss: 52.343750  [326400/401446]\n",
      "loss: 39.062500  [329600/401446]\n",
      "loss: 51.562500  [332800/401446]\n",
      "loss: 50.781250  [336000/401446]\n",
      "loss: 55.468750  [339200/401446]\n",
      "loss: 51.562500  [342400/401446]\n",
      "loss: 51.562500  [345600/401446]\n",
      "loss: 52.343750  [348800/401446]\n",
      "loss: 55.468750  [352000/401446]\n",
      "loss: 45.312500  [355200/401446]\n",
      "loss: 52.343750  [358400/401446]\n",
      "loss: 58.593750  [361600/401446]\n",
      "loss: 46.093750  [364800/401446]\n",
      "loss: 48.437500  [368000/401446]\n",
      "loss: 52.343750  [371200/401446]\n",
      "loss: 50.781250  [374400/401446]\n",
      "loss: 47.656250  [377600/401446]\n",
      "loss: 48.437500  [380800/401446]\n",
      "loss: 50.000000  [384000/401446]\n",
      "loss: 47.656250  [387200/401446]\n",
      "loss: 44.531250  [390400/401446]\n",
      "loss: 48.437500  [393600/401446]\n",
      "loss: 50.000000  [396800/401446]\n",
      "loss: 39.062500  [400000/401446]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 50.008407 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 50.781250  [    0/401446]\n",
      "loss: 48.437500  [ 3200/401446]\n",
      "loss: 52.343750  [ 6400/401446]\n",
      "loss: 59.375000  [ 9600/401446]\n",
      "loss: 50.781250  [12800/401446]\n",
      "loss: 50.781250  [16000/401446]\n",
      "loss: 50.781250  [19200/401446]\n",
      "loss: 58.593750  [22400/401446]\n",
      "loss: 47.656250  [25600/401446]\n",
      "loss: 48.437500  [28800/401446]\n",
      "loss: 42.187500  [32000/401446]\n",
      "loss: 39.843750  [35200/401446]\n",
      "loss: 46.875000  [38400/401446]\n",
      "loss: 47.656250  [41600/401446]\n",
      "loss: 45.312500  [44800/401446]\n",
      "loss: 54.687500  [48000/401446]\n",
      "loss: 54.687500  [51200/401446]\n",
      "loss: 53.906250  [54400/401446]\n",
      "loss: 51.562500  [57600/401446]\n",
      "loss: 50.000000  [60800/401446]\n",
      "loss: 51.562500  [64000/401446]\n",
      "loss: 50.000000  [67200/401446]\n",
      "loss: 51.562500  [70400/401446]\n",
      "loss: 52.343750  [73600/401446]\n",
      "loss: 50.781250  [76800/401446]\n",
      "loss: 46.093750  [80000/401446]\n",
      "loss: 39.843750  [83200/401446]\n",
      "loss: 56.250000  [86400/401446]\n",
      "loss: 59.375000  [89600/401446]\n",
      "loss: 54.687500  [92800/401446]\n",
      "loss: 52.343750  [96000/401446]\n",
      "loss: 50.000000  [99200/401446]\n",
      "loss: 45.312500  [102400/401446]\n",
      "loss: 45.312500  [105600/401446]\n",
      "loss: 49.218750  [108800/401446]\n",
      "loss: 50.000000  [112000/401446]\n",
      "loss: 49.218750  [115200/401446]\n",
      "loss: 54.687500  [118400/401446]\n",
      "loss: 49.218750  [121600/401446]\n",
      "loss: 49.218750  [124800/401446]\n",
      "loss: 51.562500  [128000/401446]\n",
      "loss: 50.781250  [131200/401446]\n",
      "loss: 47.656250  [134400/401446]\n",
      "loss: 49.218750  [137600/401446]\n",
      "loss: 57.031250  [140800/401446]\n",
      "loss: 47.656250  [144000/401446]\n",
      "loss: 46.875000  [147200/401446]\n",
      "loss: 47.656250  [150400/401446]\n",
      "loss: 45.312500  [153600/401446]\n",
      "loss: 53.125000  [156800/401446]\n",
      "loss: 44.531250  [160000/401446]\n",
      "loss: 48.437500  [163200/401446]\n",
      "loss: 53.125000  [166400/401446]\n",
      "loss: 49.218750  [169600/401446]\n",
      "loss: 46.875000  [172800/401446]\n",
      "loss: 43.750000  [176000/401446]\n",
      "loss: 52.343750  [179200/401446]\n",
      "loss: 55.468750  [182400/401446]\n",
      "loss: 53.125000  [185600/401446]\n",
      "loss: 50.781250  [188800/401446]\n",
      "loss: 50.781250  [192000/401446]\n",
      "loss: 55.468750  [195200/401446]\n",
      "loss: 57.031250  [198400/401446]\n",
      "loss: 47.656250  [201600/401446]\n",
      "loss: 45.312500  [204800/401446]\n",
      "loss: 55.468750  [208000/401446]\n",
      "loss: 42.968750  [211200/401446]\n",
      "loss: 50.000000  [214400/401446]\n",
      "loss: 57.812500  [217600/401446]\n",
      "loss: 48.437500  [220800/401446]\n",
      "loss: 46.875000  [224000/401446]\n",
      "loss: 54.687500  [227200/401446]\n",
      "loss: 50.781250  [230400/401446]\n",
      "loss: 46.875000  [233600/401446]\n",
      "loss: 55.468750  [236800/401446]\n",
      "loss: 51.562500  [240000/401446]\n",
      "loss: 44.531250  [243200/401446]\n",
      "loss: 54.687500  [246400/401446]\n",
      "loss: 49.218750  [249600/401446]\n",
      "loss: 44.531250  [252800/401446]\n",
      "loss: 42.187500  [256000/401446]\n",
      "loss: 53.125000  [259200/401446]\n",
      "loss: 52.343750  [262400/401446]\n",
      "loss: 52.343750  [265600/401446]\n",
      "loss: 46.093750  [268800/401446]\n",
      "loss: 50.781250  [272000/401446]\n",
      "loss: 50.781250  [275200/401446]\n",
      "loss: 50.781250  [278400/401446]\n",
      "loss: 47.656250  [281600/401446]\n",
      "loss: 56.250000  [284800/401446]\n",
      "loss: 48.437500  [288000/401446]\n",
      "loss: 46.093750  [291200/401446]\n",
      "loss: 50.000000  [294400/401446]\n",
      "loss: 46.093750  [297600/401446]\n",
      "loss: 46.875000  [300800/401446]\n",
      "loss: 46.875000  [304000/401446]\n",
      "loss: 57.812500  [307200/401446]\n",
      "loss: 46.093750  [310400/401446]\n",
      "loss: 46.875000  [313600/401446]\n",
      "loss: 51.562500  [316800/401446]\n",
      "loss: 50.781250  [320000/401446]\n",
      "loss: 55.468750  [323200/401446]\n",
      "loss: 52.343750  [326400/401446]\n",
      "loss: 52.343750  [329600/401446]\n",
      "loss: 52.343750  [332800/401446]\n",
      "loss: 50.781250  [336000/401446]\n",
      "loss: 49.218750  [339200/401446]\n",
      "loss: 45.312500  [342400/401446]\n",
      "loss: 52.343750  [345600/401446]\n",
      "loss: 47.656250  [348800/401446]\n",
      "loss: 50.000000  [352000/401446]\n",
      "loss: 47.656250  [355200/401446]\n",
      "loss: 46.093750  [358400/401446]\n",
      "loss: 58.593750  [361600/401446]\n",
      "loss: 52.343750  [364800/401446]\n",
      "loss: 51.562500  [368000/401446]\n",
      "loss: 55.468750  [371200/401446]\n",
      "loss: 47.656250  [374400/401446]\n",
      "loss: 56.250000  [377600/401446]\n",
      "loss: 47.656250  [380800/401446]\n",
      "loss: 50.781250  [384000/401446]\n",
      "loss: 53.906250  [387200/401446]\n",
      "loss: 48.437500  [390400/401446]\n",
      "loss: 57.031250  [393600/401446]\n",
      "loss: 49.218750  [396800/401446]\n",
      "loss: 49.218750  [400000/401446]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 50.008407 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 50.781250  [    0/401446]\n",
      "loss: 50.781250  [ 3200/401446]\n",
      "loss: 51.562500  [ 6400/401446]\n",
      "loss: 54.687500  [ 9600/401446]\n",
      "loss: 53.906250  [12800/401446]\n",
      "loss: 50.000000  [16000/401446]\n",
      "loss: 51.562500  [19200/401446]\n",
      "loss: 51.562500  [22400/401446]\n",
      "loss: 46.875000  [25600/401446]\n",
      "loss: 49.218750  [28800/401446]\n",
      "loss: 54.687500  [32000/401446]\n",
      "loss: 49.218750  [35200/401446]\n",
      "loss: 43.750000  [38400/401446]\n",
      "loss: 50.781250  [41600/401446]\n",
      "loss: 44.531250  [44800/401446]\n",
      "loss: 52.343750  [48000/401446]\n",
      "loss: 53.125000  [51200/401446]\n",
      "loss: 53.125000  [54400/401446]\n",
      "loss: 39.843750  [57600/401446]\n",
      "loss: 50.000000  [60800/401446]\n",
      "loss: 49.218750  [64000/401446]\n",
      "loss: 50.000000  [67200/401446]\n",
      "loss: 50.781250  [70400/401446]\n",
      "loss: 43.750000  [73600/401446]\n",
      "loss: 55.468750  [76800/401446]\n",
      "loss: 50.000000  [80000/401446]\n",
      "loss: 52.343750  [83200/401446]\n",
      "loss: 48.437500  [86400/401446]\n",
      "loss: 48.437500  [89600/401446]\n",
      "loss: 48.437500  [92800/401446]\n",
      "loss: 50.000000  [96000/401446]\n",
      "loss: 47.656250  [99200/401446]\n",
      "loss: 58.593750  [102400/401446]\n",
      "loss: 44.531250  [105600/401446]\n",
      "loss: 48.437500  [108800/401446]\n",
      "loss: 50.000000  [112000/401446]\n",
      "loss: 46.875000  [115200/401446]\n",
      "loss: 47.656250  [118400/401446]\n",
      "loss: 48.437500  [121600/401446]\n",
      "loss: 52.343750  [124800/401446]\n",
      "loss: 42.187500  [128000/401446]\n",
      "loss: 54.687500  [131200/401446]\n",
      "loss: 49.218750  [134400/401446]\n",
      "loss: 50.781250  [137600/401446]\n",
      "loss: 50.781250  [140800/401446]\n",
      "loss: 43.750000  [144000/401446]\n",
      "loss: 40.625000  [147200/401446]\n",
      "loss: 50.781250  [150400/401446]\n",
      "loss: 46.875000  [153600/401446]\n",
      "loss: 53.125000  [156800/401446]\n",
      "loss: 49.218750  [160000/401446]\n",
      "loss: 46.093750  [163200/401446]\n",
      "loss: 50.000000  [166400/401446]\n",
      "loss: 52.343750  [169600/401446]\n",
      "loss: 55.468750  [172800/401446]\n",
      "loss: 49.218750  [176000/401446]\n",
      "loss: 47.656250  [179200/401446]\n",
      "loss: 47.656250  [182400/401446]\n",
      "loss: 43.750000  [185600/401446]\n",
      "loss: 46.093750  [188800/401446]\n",
      "loss: 45.312500  [192000/401446]\n",
      "loss: 51.562500  [195200/401446]\n",
      "loss: 46.093750  [198400/401446]\n",
      "loss: 51.562500  [201600/401446]\n",
      "loss: 47.656250  [204800/401446]\n",
      "loss: 57.031250  [208000/401446]\n",
      "loss: 49.218750  [211200/401446]\n",
      "loss: 47.656250  [214400/401446]\n",
      "loss: 45.312500  [217600/401446]\n",
      "loss: 46.093750  [220800/401446]\n",
      "loss: 51.562500  [224000/401446]\n",
      "loss: 45.312500  [227200/401446]\n",
      "loss: 47.656250  [230400/401446]\n",
      "loss: 48.437500  [233600/401446]\n",
      "loss: 49.218750  [236800/401446]\n",
      "loss: 45.312500  [240000/401446]\n",
      "loss: 50.781250  [243200/401446]\n",
      "loss: 48.437500  [246400/401446]\n",
      "loss: 60.156250  [249600/401446]\n",
      "loss: 51.562500  [252800/401446]\n",
      "loss: 48.437500  [256000/401446]\n",
      "loss: 51.562500  [259200/401446]\n",
      "loss: 49.218750  [262400/401446]\n",
      "loss: 49.218750  [265600/401446]\n",
      "loss: 53.906250  [268800/401446]\n",
      "loss: 50.000000  [272000/401446]\n",
      "loss: 47.656250  [275200/401446]\n",
      "loss: 44.531250  [278400/401446]\n",
      "loss: 46.093750  [281600/401446]\n",
      "loss: 49.218750  [284800/401446]\n",
      "loss: 47.656250  [288000/401446]\n",
      "loss: 48.437500  [291200/401446]\n",
      "loss: 45.312500  [294400/401446]\n",
      "loss: 46.093750  [297600/401446]\n",
      "loss: 50.000000  [300800/401446]\n",
      "loss: 47.656250  [304000/401446]\n",
      "loss: 47.656250  [307200/401446]\n",
      "loss: 60.937500  [310400/401446]\n",
      "loss: 56.250000  [313600/401446]\n",
      "loss: 38.281250  [316800/401446]\n",
      "loss: 44.531250  [320000/401446]\n",
      "loss: 52.343750  [323200/401446]\n",
      "loss: 50.000000  [326400/401446]\n",
      "loss: 52.343750  [329600/401446]\n",
      "loss: 46.875000  [332800/401446]\n",
      "loss: 53.125000  [336000/401446]\n",
      "loss: 50.000000  [339200/401446]\n",
      "loss: 46.875000  [342400/401446]\n",
      "loss: 53.906250  [345600/401446]\n",
      "loss: 43.750000  [348800/401446]\n",
      "loss: 55.468750  [352000/401446]\n",
      "loss: 50.781250  [355200/401446]\n",
      "loss: 53.906250  [358400/401446]\n",
      "loss: 50.781250  [361600/401446]\n",
      "loss: 52.343750  [364800/401446]\n",
      "loss: 49.218750  [368000/401446]\n",
      "loss: 51.562500  [371200/401446]\n",
      "loss: 49.218750  [374400/401446]\n",
      "loss: 63.281250  [377600/401446]\n",
      "loss: 50.000000  [380800/401446]\n",
      "loss: 52.343750  [384000/401446]\n",
      "loss: 49.218750  [387200/401446]\n",
      "loss: 56.250000  [390400/401446]\n",
      "loss: 50.781250  [393600/401446]\n",
      "loss: 48.437500  [396800/401446]\n",
      "loss: 51.562500  [400000/401446]\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 50.008946 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ]
}